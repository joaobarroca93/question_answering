{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "In this notebook we will define and test some of the metrics that we will implement in our evaluation pipeline.\n",
    "\n",
    "We can use the `rank_eval` package (https://pypi.org/project/rank-eval/) that was featured in ECIR 2022. This package implements some standard metrics that we can use for ranking such as:\n",
    "- Mean Average Precision (mAP)\n",
    "- Normalized Discounted Cumulative Gain (NDCG)\n",
    "- Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started with `rank_eval` lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7861261099276952\n",
      "{'map@5': 0.6416666666666666, 'mrr': 0.75}\n",
      "{'ndcg@5': 0.7861261099276952, 'map@5': 0.6416666666666666, 'mrr': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ndcg@5': {'q_1': 0.9430144683295216, 'q_2': 0.6292377515258687},\n",
       " 'map@5': {'q_1': 0.8333333333333333, 'q_2': 0.45},\n",
       " 'mrr': {'q_1': 1.0, 'q_2': 0.5}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_eval import Qrels, Run, evaluate, compare\n",
    "\n",
    "qrels = Qrels()\n",
    "qrels.add_multi(\n",
    "    q_ids=[\"q_1\", \"q_2\"],\n",
    "    doc_ids=[\n",
    "        [\"doc_12\", \"doc_25\"],  # q_1 relevant documents\n",
    "        [\"doc_11\", \"doc_2\"],  # q_2 relevant documents\n",
    "    ],\n",
    "    scores=[\n",
    "        [5.0, 3.0],  # documents relevance (can be a relevance score, or a binary score of 1 indicating that the document is relevant)\n",
    "        [6.0, 1.0],  # documents relevance\n",
    "    ],\n",
    ")\n",
    "\n",
    "run = Run()\n",
    "run.add_multi(\n",
    "    q_ids=[\"q_1\", \"q_2\"],\n",
    "    doc_ids=[\n",
    "        [\"doc_12\", \"doc_23\", \"doc_25\", \"doc_36\", \"doc_32\", \"doc_35\"],\n",
    "        [\"doc_12\", \"doc_11\", \"doc_25\", \"doc_36\", \"doc_2\",  \"doc_35\"],\n",
    "    ],\n",
    "    scores=[\n",
    "        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compute score for a single metric\n",
    "print(evaluate(qrels, run, \"ndcg@5\"))\n",
    "\n",
    "# Compute scores for multiple metrics at once\n",
    "print(evaluate(qrels, run, [\"map@5\", \"mrr\"]))\n",
    "\n",
    "# Computed metric scores are saved in the Run object\n",
    "print(run.mean_scores)\n",
    "\n",
    "# Access scores for each query\n",
    "dict(run.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#    Model      MAP@3    MRR@3    NDCG@3\n",
      "---  -------  -------  -------  --------\n",
      "a    model_1   0.6667   0.6667      0.75\n",
      "b    model_2   0.5      0.5         0.5\n"
     ]
    }
   ],
   "source": [
    "from rank_eval import compare\n",
    "\n",
    "# Compare different runs and perform statistical tests\n",
    "# TODO: which statistical tests are they performing?\n",
    "\n",
    "qrels = Qrels()\n",
    "qrels.add_multi(\n",
    "    q_ids=[\"q_1\", \"q_2\"],\n",
    "    doc_ids=[\n",
    "        [\"doc_1\"],\n",
    "        [\"doc_2\"],\n",
    "    ],\n",
    "    scores=[\n",
    "        [1],\n",
    "        [1],\n",
    "    ],\n",
    ")\n",
    "\n",
    "run_1 = Run()\n",
    "run_1.name = \"model_1\"\n",
    "run_1.add_multi(\n",
    "    q_ids=[\"q_1\", \"q_2\"],\n",
    "    doc_ids=[\n",
    "        [\"doc_1\", \"doc_11\", \"doc_111\"],\n",
    "        [\"doc_222\", \"doc_22\", \"doc_2\"],\n",
    "    ],\n",
    "    scores=[\n",
    "        [0.9, 0.8, 0.7],\n",
    "        [0.9, 0.8, 0.7],\n",
    "    ],\n",
    ")\n",
    "\n",
    "run_2 = Run()\n",
    "run_2.name = \"model_2\"\n",
    "run_2.add_multi(\n",
    "    q_ids=[\"q_1\", \"q_2\"],\n",
    "    doc_ids=[\n",
    "        [\"doc_1\", \"doc_11\", \"doc_111\"],\n",
    "        [\"doc_22\", \"doc_222\", \"doc_2222\"],\n",
    "    ],\n",
    "    scores=[\n",
    "        [0.9, 0.8, 0.7],\n",
    "        [0.9, 0.8, 0.7],\n",
    "    ],\n",
    ")\n",
    "\n",
    "runs = [run_1, run_2]\n",
    "\n",
    "\n",
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=runs,\n",
    "    metrics=[\"map@3\", \"mrr@3\", \"ndcg@3\"],\n",
    "    max_p=0.05/len(runs)\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the `RankMetrics` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import RankMetrics\n",
    "metrics = RankMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"q_1\": [\n",
    "        (\"d_1\", 1)\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 1)\n",
    "    ]\n",
    "}\n",
    "predictions = {\n",
    "    \"q_1\": [\n",
    "        (\"d_1\", 0.9),\n",
    "        (\"doc_11\", 0.8),\n",
    "        (\"doc_111\", 0.7)\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"doc_222\", 0.9),\n",
    "        (\"doc_22\", 0.8),\n",
    "        (\"doc_2\", 0.7)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map@3': 0.5, 'mrr@3': 0.5, 'ndcg@3': 0.5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default (supported) metrics are \"map\", \"mrr\", and \"ndcg\"\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map@1': 0.5, 'mrr@1': 0.5, 'ndcg@1': 0.5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default top_k is 1\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@3': 0.5, 'map@3': 0.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can ask for a subset of metrics only\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3, metrics=[\"ndcg\", \"map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we request only one metric, it outputs the value directly\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3, metrics=[\"ndcg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric `dummy` is not supported. Currently, the following metrics are supported: ['map', 'mrr', 'ndcg']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# if we ask for a not supported metric, it gives an error\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m metrics\u001b[39m.\u001b[39;49mcompute_metrics(ground_truth\u001b[39m=\u001b[39;49mground_truth, predictions\u001b[39m=\u001b[39;49mpredictions, metrics\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mdummy\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/projects/deus-use-case/src/metrics/rank.py:17\u001b[0m, in \u001b[0;36mRankMetrics.compute_metrics\u001b[0;34m(self, ground_truth, predictions, top_k, metrics)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(\n\u001b[1;32m     11\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     12\u001b[0m         ground_truth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         metrics: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m], \u001b[39mfloat\u001b[39m]:\n\u001b[0;32m---> 17\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_metrics(metrics) \u001b[39mif\u001b[39;00m metrics \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m     18\u001b[0m     qrels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_ground_truth(ground_truth\u001b[39m=\u001b[39mground_truth)\n\u001b[1;32m     19\u001b[0m     run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_predictions(predictions\u001b[39m=\u001b[39mpredictions)\n",
      "File \u001b[0;32m~/Desktop/projects/deus-use-case/src/metrics/rank.py:26\u001b[0m, in \u001b[0;36mRankMetrics._validate_metrics\u001b[0;34m(self, metrics)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m metric \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe metric `\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m` is not supported. Currently, the following\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m metrics are supported: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m metrics\n",
      "\u001b[0;31mValueError\u001b[0m: The metric `dummy` is not supported. Currently, the following metrics are supported: ['map', 'mrr', 'ndcg']"
     ]
    }
   ],
   "source": [
    "# if we ask for a not supported metric, it gives an error\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, metrics=[\"dummy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate that metrics are being correctly computed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRR\n",
    "\n",
    "We are expecting:\n",
    "- q1 -> 1/3 (1st relevant item in the 3rd position)\n",
    "- q2 -> 1/1 (1st relevant item in the 1st position)\n",
    "- mrr = (1/3 + 1) / 2 = 0.667\n",
    "\n",
    "*Note that the scores do not have any influence in this particular metric.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = {\n",
    "    \"q_1\": [\n",
    "        (\"d_1\", 1),\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 1)\n",
    "    ],\n",
    "}\n",
    "predictions = {\n",
    "    \"q_1\": [\n",
    "        (\"d_11\", 0.0),\n",
    "        (\"d_111\", 0.0),\n",
    "        (\"d_1\", 0.0)\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 0.9),\n",
    "        (\"d_22\", 0.8),\n",
    "        (\"d_222\", 0.7),\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3, metrics=[\"mrr\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP\n",
    "\n",
    "We are expecting:\n",
    "- q1 -> AP@3 = (1/1 + 2/3)/2 = 0.833\n",
    "- q2 -> AP@3 = 1/1 = 1\n",
    "- mAP = (1 + 1/3) / 2 = 0.917\n",
    "\n",
    "*Note that the scores do not have any influence in this particular metric.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = {\n",
    "    \"q_1\": [\n",
    "        (\"d_1\", 1),\n",
    "        (\"d_11\", 1),\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 1)\n",
    "    ],\n",
    "}\n",
    "predictions = {\n",
    "    \"q_1\": [\n",
    "        (\"d_11\", 0.0),\n",
    "        (\"d_111\", 0.0),\n",
    "        (\"d_1\", 0.0)\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 0.9),\n",
    "        (\"d_22\", 0.8),\n",
    "        (\"d_222\", 0.7),\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3, metrics=[\"map\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG\n",
    "\n",
    "We are expecting:\n",
    "- q1 -> DCG@3 = (0.7/log(1+3)), IDCG@3 = (0.7/log(1+1)), NDCG@3 = DCG@3/IDCG@3 = 0.5\n",
    "- q2 -> DCG@3 = (0.7/log(1)), IDCG@3 = (0.7/log(1)), NDCG@3 = DCG@3/IDCG@3 = 1\n",
    "- NDCG = (0.5 + 1) / 2 = 0.667\n",
    "\n",
    "*Note that, now, the scores **affect** the metric.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = {\n",
    "    \"q_1\": [\n",
    "        (\"d_1\", 1),\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 1)\n",
    "    ],\n",
    "}\n",
    "predictions = {\n",
    "    \"q_1\": [\n",
    "        (\"d_11\", 0.9),\n",
    "        (\"d_111\", 0.8),\n",
    "        (\"d_1\", 0.7)\n",
    "    ],\n",
    "    \"q_2\": [\n",
    "        (\"d_2\", 0.9),\n",
    "        (\"d_22\", 0.8),\n",
    "        (\"d_222\", 0.7),\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics.compute_metrics(ground_truth=ground_truth, predictions=predictions, top_k=3, metrics=[\"map\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
